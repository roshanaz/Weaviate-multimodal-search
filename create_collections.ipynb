{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.config import Configure, Property, DataType, Multi2VecField\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_data(image_path):\n",
    "    \"\"\"Read image as base64 and PIL object, caching to avoid redundant reads\"\"\"\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        raw_data = img_file.read()\n",
    "        base64_data = base64.b64encode(raw_data).decode(\"utf-8\")\n",
    "        pil_img = Image.open(io.BytesIO(raw_data))\n",
    "    return base64_data, pil_img\n",
    "\n",
    "def process_metadata(metadata, max_chars=200):\n",
    "    \"\"\"Process captions from metadata\"\"\"\n",
    "    captions = metadata.get(\"captions\", [])\n",
    "    concatenated = \" \".join(captions)[:max_chars] if captions else \"No captions available\"\n",
    "    return concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flickr_schema_multi2vec(client, collection_name, image_weight=0.5):\n",
    "    \"\"\"Create schema for multi2vec-clip\"\"\"\n",
    "    if client.collections.exists(collection_name):\n",
    "        client.collections.delete(collection_name)\n",
    "        print(f\"existing {collection_name} deleted, creating a new one.\")\n",
    "    \n",
    "    image_weight = round(image_weight, 2)\n",
    "    text_weight = round(1.0 - image_weight, 2)\n",
    "\n",
    "    client.collections.create(\n",
    "        collection_name,\n",
    "        properties=[\n",
    "            Property(name=\"image\", data_type=DataType.BLOB),\n",
    "            Property(name=\"image_id\", data_type=DataType.TEXT),\n",
    "            Property(name=\"captions\", data_type=DataType.TEXT),\n",
    "        ],\n",
    "        vectorizer_config=[\n",
    "            Configure.NamedVectors.multi2vec_clip(\n",
    "                name=\"image_vector\",\n",
    "                image_fields=[Multi2VecField(name=\"image\", weight=image_weight)],\n",
    "                text_fields=[Multi2VecField(name=\"captions\", weight=text_weight)]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Created {collection_name} schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data to Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data_multi2vec(client, data_dir, collection_name, batch_size=100):\n",
    "    \"\"\"Import data with multi2vec-clip\"\"\"\n",
    "    collection = client.collections.get(collection_name)\n",
    "    images_dir = os.path.join(data_dir, \"images\")\n",
    "    metadata_dir = os.path.join(data_dir, \"metadata\")\n",
    "    metadata_files = [f for f in os.listdir(metadata_dir) if f.endswith('.json')]\n",
    "    print(f\"Importing {len(metadata_files)} images for multi2vec\")\n",
    "\n",
    "    batch_errors = []\n",
    "    with collection.batch.fixed_size(batch_size=batch_size) as batch:\n",
    "        for metadata_file in tqdm(metadata_files):\n",
    "            try:\n",
    "                with open(os.path.join(metadata_dir, metadata_file), 'r') as f:\n",
    "                    metadata = json.load(f)\n",
    "                \n",
    "                image_path = os.path.join(images_dir, f\"image_{metadata['image_id']}.jpg\")\n",
    "                image_data, _ = read_image_data(image_path)\n",
    "                captions = process_metadata(metadata)\n",
    "                \n",
    "                batch.add_object(\n",
    "                    properties={\n",
    "                        \"image\": image_data,\n",
    "                        \"image_id\": metadata[\"image_id\"],\n",
    "                        \"captions\": captions\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                batch_errors.append(f\"Error processing {metadata_file}: {str(e)}\")\n",
    "                continue\n",
    "    if batch_errors:\n",
    "        print(f\"Encountered {len(batch_errors)} errors during batch import\")\n",
    "        for err in batch_errors[:5]:  # Print first 5 errors\n",
    "            print(err)\n",
    "    print(\"Finished multi2vec import\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Weaviate\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "data_dir = \"/Users/roshanakzakizadeh/Projects/multimodal_comparison/flickr30k_sample\"\n",
    "\n",
    "create_flickr_schema_multi2vec(client, collection_name=\"Flickr30k_multi2vec\", image_weight=0.5)\n",
    "\n",
    "import_data_multi2vec(client, data_dir, collection_name=\"Flickr30k_multi2vec\")\n",
    "\n",
    "client.close ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate.classes as wvc\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_text_search(collection_name: str, queries: list[str], k: int):\n",
    "    \"\"\"\n",
    "    Runs query search inside the given collection\n",
    "    Args:\n",
    "        collection_name: name of the collection on Weaviate\n",
    "        queries: list of words or phrases\n",
    "        k : number of items to retrieve from the collection\n",
    "    Returns:\n",
    "        Dictionary containg search results with latency, image_ids, caption and similarities\n",
    "    \n",
    "    \"\"\"\n",
    "    client = weaviate.connect_to_local()\n",
    "    collection = client.collections.get(collection_name)\n",
    "    response = collection.aggregate.over_all(total_count=True)\n",
    "    print(f\"{collection_name} collection size is : {response.total_count}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for query in queries:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = collection.query.near_text(\n",
    "                query=query,\n",
    "                limit=k,\n",
    "                return_metadata=wvc.query.MetadataQuery(distance=True)\n",
    "            )        \n",
    "\n",
    "            latency = time.time() - start_time\n",
    "\n",
    "            similarities = [obj.metadata.distance for obj in response.objects]\n",
    "            print(f\"Query: {query}, Similarities: {similarities}\")\n",
    "            retrieved_ids = [obj.properties['image_id'] for obj in response.objects]\n",
    "            retrieved_captions = [obj.properties['captions'] for obj in response.objects]\n",
    "\n",
    "            results[query] = {\n",
    "                \"latency\": latency,\n",
    "                \"image_ids\": retrieved_ids,\n",
    "                \"captions\": retrieved_captions,\n",
    "                \"similarities\": similarities\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"error processing query {query} in {collection_name}: {str(e)}\")\n",
    "            results[query] = {\n",
    "                \"latency\": 0.0,\n",
    "                \"image_ids\": [],\n",
    "                \"captions\": [],\n",
    "                \"similarities\": []\n",
    "            }\n",
    "    \n",
    "    client.close()\n",
    "    return results\n",
    "\n",
    "def query_hybrid_search(collection_name: str, text_query: str, image_path: str, k: int, text_weight: float):\n",
    "    \"\"\"\n",
    "    Combines text and image search in the given collection using multi2vec-clip\n",
    "    \n",
    "    Args:\n",
    "        collection_name: name of the collection on Weaviate\n",
    "        text_query: text to search for\n",
    "        image_path: path to the query image\n",
    "        k: number of items to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing search results with latency, image_ids, captions, and similarities\n",
    "    \"\"\"\n",
    "    client = weaviate.connect_to_local()\n",
    "    collection = client.collections.get(collection_name)\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # First try text search\n",
    "        text_response = collection.query.near_text(\n",
    "            query=text_query,\n",
    "            limit=k,\n",
    "            return_metadata=wvc.query.MetadataQuery(distance=True)\n",
    "        )\n",
    "        \n",
    "        # Then try image search\n",
    "        image_response = collection.query.near_image(\n",
    "            near_image=Path(image_path),\n",
    "            limit=k,\n",
    "            return_metadata=wvc.query.MetadataQuery(distance=True)\n",
    "        )\n",
    "        \n",
    "   \n",
    "         # Get max distances for normalization\n",
    "        text_max = max(obj.metadata.distance for obj in text_response.objects)\n",
    "        image_max = max(obj.metadata.distance for obj in image_response.objects)\n",
    "\n",
    "\n",
    "        # Combine and rank results\n",
    "        combined_results = {}\n",
    "        \n",
    "        # First, collect all results and their normalized distances\n",
    "        for obj in text_response.objects:\n",
    "            normalized_distance = (obj.metadata.distance / text_max)\n",
    "            combined_results[obj.properties['image_id']] = {\n",
    "                'text_distance': normalized_distance,\n",
    "                'image_distance': 1.0,  # Default max distance if not found in image search\n",
    "                'properties': obj.properties,\n",
    "                'count': 1\n",
    "            }\n",
    "            \n",
    "        for obj in image_response.objects:\n",
    "            img_id = obj.properties['image_id']\n",
    "            normalized_distance = (obj.metadata.distance / image_max)\n",
    "            if img_id in combined_results:\n",
    "                combined_results[img_id]['image_distance'] = normalized_distance\n",
    "                combined_results[img_id]['count'] = 2\n",
    "            else:\n",
    "                combined_results[img_id] = {\n",
    "                    'text_distance': 1.0,  # Default max distance if not found in text search\n",
    "                    'image_distance': normalized_distance,\n",
    "                    'properties': obj.properties,\n",
    "                    'count': 1\n",
    "                }\n",
    "\n",
    "        # Calculate final scores using a more sophisticated ranking\n",
    "        for img_id in combined_results:\n",
    "            result = combined_results[img_id]\n",
    "            \n",
    "            # Define a threshold for considering a weight as effectively 0 or 1\n",
    "            EPSILON = 1e-10  # Very small number\n",
    "\n",
    "            if text_weight < EPSILON:\n",
    "                # Pure image search - only consider images that appear in image search\n",
    "                if img_id in [obj.properties['image_id'] for obj in image_response.objects]:\n",
    "                    result['final_score'] = result['image_distance']\n",
    "                else:\n",
    "                    result['final_score'] = float('inf')  # Exclude from results\n",
    "            elif text_weight > (1 - EPSILON):\n",
    "                # Pure text search - only consider images that appear in text search\n",
    "                if img_id in [obj.properties['image_id'] for obj in text_response.objects]:\n",
    "                    result['final_score'] = result['text_distance']\n",
    "                else:\n",
    "                    result['final_score'] = float('inf')  # Exclude from results\n",
    "            else:\n",
    "                # Hybrid search\n",
    "                text_score = result['text_distance'] * text_weight\n",
    "                image_score = result['image_distance'] * (1.0 - text_weight)\n",
    "                bonus = 0.2 if result['count'] == 2 else 0\n",
    "                result['final_score'] = (text_score + image_score) * (1.0 - bonus)\n",
    "\n",
    "        # Sort by final score\n",
    "        sorted_results = sorted(\n",
    "            combined_results.items(), \n",
    "            key=lambda x: x[1]['final_score']\n",
    "        )[:k]\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Process results\n",
    "        retrieved_ids = [item[0] for item in sorted_results]\n",
    "        retrieved_captions = [item[1]['properties']['captions'] for item in sorted_results]\n",
    "        similarities = [item[1]['final_score'] for item in sorted_results]\n",
    "\n",
    "        # Print final combined distances\n",
    "        print(\"\\nFinal scores:\")\n",
    "        for item in sorted_results:\n",
    "            print(f\"ID: {item[0]}, Score: {item[1]['final_score']:.3f}\")\n",
    "            print(f\"  Text distance: {item[1]['text_distance']:.3f}\")\n",
    "            print(f\"  Image distance: {item[1]['image_distance']:.3f}\")\n",
    "            print(f\"  Count: {item[1]['count']}\")\n",
    "        \n",
    "        \n",
    "        results = {\n",
    "            text_query: {\n",
    "                \"latency\": latency,\n",
    "                \"image_ids\": retrieved_ids,\n",
    "                \"captions\": retrieved_captions,\n",
    "                \"similarities\": similarities\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in hybrid search: {str(e)}\")\n",
    "        return {\n",
    "            text_query: {\n",
    "                \"latency\": 0.0,\n",
    "                \"image_ids\": [],\n",
    "                \"captions\": [],\n",
    "                \"similarities\": []\n",
    "            }\n",
    "        }\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def query_by_image(collection_name: str, image_path: str, k: int):\n",
    "    \"\"\"\n",
    "    Search using only image in the given collection\n",
    "    \n",
    "    Args:\n",
    "        collection_name: name of the collection on Weaviate\n",
    "        image_path: path to the query image\n",
    "        k: number of items to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing search results with latency, image_ids, captions, and similarities\n",
    "    \"\"\"\n",
    "    client = weaviate.connect_to_local()\n",
    "    collection = client.collections.get(collection_name)\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = collection.query.near_image(\n",
    "            near_image=Path(image_path),\n",
    "            limit=k,\n",
    "            return_metadata=wvc.query.MetadataQuery(distance=True)\n",
    "        )\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Process results\n",
    "        similarities = [obj.metadata.distance for obj in response.objects]\n",
    "        retrieved_ids = [obj.properties['image_id'] for obj in response.objects]\n",
    "        retrieved_captions = [obj.properties['captions'] for obj in response.objects]\n",
    "        print(f\"image search similarities: {similarities}\")\n",
    "        \n",
    "        results = {\n",
    "            \"image_search\": {\n",
    "                \"latency\": latency,\n",
    "                \"image_ids\": retrieved_ids,\n",
    "                \"captions\": retrieved_captions,\n",
    "                \"similarities\": similarities\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in image search: {str(e)}\")\n",
    "        return {\n",
    "            \"image_search\": {\n",
    "                \"latency\": 0.0,\n",
    "                \"image_ids\": [],\n",
    "                \"captions\": [],\n",
    "                \"similarities\": []\n",
    "            }\n",
    "        }\n",
    "    finally:\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_search_results(results: dict, query: str, images_dir: str):\n",
    "    \"\"\"\n",
    "    displays images along with their captions.\n",
    "    \"\"\"\n",
    "    print(f\"Images retrieved for the query {query}:\")\n",
    "\n",
    "    for img_id in results[query]['image_ids']:\n",
    "        image_path = os.path.join(images_dir, \"image_\" + img_id + '.jpg')\n",
    "        img = Image.open(image_path)\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "\n",
    "        caption = results[query]['captions'][results[query]['image_ids'].index(img_id)]\n",
    "        plt.title(f'Image ID {img_id}. Caption: {caption}', wrap=True)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "def visualize_search_results(results: dict, query: str, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Create a grid visualization of results with metrics\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, top_k, figsize=(15, 5))\n",
    "    fig.suptitle(f'Results for query: \"{query}\"')\n",
    "    \n",
    "    for idx, (img_id, caption, distance) in enumerate(zip(\n",
    "        results[query]['image_ids'][:top_k],\n",
    "        results[query]['captions'][:top_k],\n",
    "        results[query]['similarities'][:top_k]\n",
    "    )):\n",
    "        # Display image\n",
    "        axes[idx].imshow(Image.open(f\"{data_dir}/images/image_{img_id}.jpg\"))\n",
    "        axes[idx].set_title(f'Distance: {distance:.3f}')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_text = \"a dog playing in the park\"\n",
    "# query_image_path = \"./weaviate_data/image_queries/dog-4362595_640.jpg\"\n",
    "# query_text = \"Children in colorful raincoats splashing in puddles\"\n",
    "# query_image_path = \"./weaviate_data/image_queries/child-504321_640.jpg\"\n",
    "query_text = \"a close-up of hands creating traditional handicrafts\"\n",
    "query_image_path = \"./weaviate_data/image_queries/clay-1139098_640.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the captions run display_search_results\n",
    "# text search\n",
    "collection_name = \"Flickr30k_multi2vec\"\n",
    "results = query_text_search(collection_name=collection_name, queries=[query_text], k=3)\n",
    "display_search_results(results=results, query=query_text, images_dir=data_dir+'/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image search\n",
    "collection_name = \"Flickr30k_multi2vec\"\n",
    "    \n",
    "# Test image-only search\n",
    "image_results = query_by_image(\n",
    "    collection_name=collection_name,\n",
    "    image_path=query_image_path,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "display_search_results(image_results, \"image_search\", images_dir=data_dir+'/images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid search\n",
    "# Test hybrid search\n",
    "hybrid_results = query_hybrid_search(\n",
    "    collection_name=collection_name,\n",
    "    text_query=query_text,\n",
    "    image_path=query_image_path,\n",
    "    k=3,\n",
    "    text_weight=0.7\n",
    ")\n",
    "\n",
    "# Display results using the visualization function you already have\n",
    "display_search_results(hybrid_results, query_text , images_dir=data_dir+'/images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
